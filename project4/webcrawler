#!/usr/bin/python

# imports
import socket
import urlparse
import re
import sys
import time
from HTMLParser import HTMLParser

# global variables
USERNAME = sys.argv[1]
PASSWORD = sys.argv[2]
PORT = 80
ROOT = "fring.ccs.neu.edu"
CSRF = ""
session = ""
pages_visited = []
pages_to_visit = []
found_headers = []
socket.setdefaulttimeout = 10
CRLF = "\r\n\r\n"
NL = "\r\n"

# create an HTMLParser subclass and override the handler methods to find <a> and <h2> tags
class MyHTMLParser(HTMLParser):
  def __init__(self):
    HTMLParser.__init__(self)
    # initialize the readData variable to false so we don't read the data of all tags
    self.readData = False

  # Parse data within start tags in raw HTML
  def handle_starttag(self, tag, attrs):
    global pages_to_visit, pages_visited, found_headers
    # if we encounter an <a> tag, add its href to the work queue
    if tag == 'a':
      # this is a series of safe accesses to the attrs list seeing if href exists
      newkeys = [i[0] for i in attrs]
      try:
        hrefIndex = newkeys.index('href')
        if hrefIndex >= 0:
          i = attrs[hrefIndex][1]
          try:
            pages_visited.index(i)
          except:
            # if href exists and its path is not in our list of visited paths,
            # add it to our work queue if it's a path on fring.ccs.neu.edu
            if re.match(r"(/fakebook.*/)", i):
              pages_to_visit.append(i)
      except:
        pass
    # if we encounter an <h2> tag with the 'secret_flag' class,
    # set self.readData to true so that we extract the flag
    elif tag == 'h2':
      newkeys = [i[0] for i in attrs]
      try:
        classIndex = newkeys.index('class')
        if classIndex >= 0:
          i = attrs[classIndex][1]
          if i == 'secret_flag':
            self.readData = True
      except:
        pass

  # Pull flag out and store it
  def handle_data(self, data):
    global found_headers
    # if self.readData is true, extract the flag and add it to our list
    # if we haven't found this flag already
    if self.readData:
      self.readData = False
      flag_regex = r"FLAG: (.*)"
      matches = re.search(flag_regex, data)
      header = matches.group(1)
      if header not in found_headers:
        found_headers.append(header)


# instantiate the parser to be used globally
parser = MyHTMLParser()

# main method, crawls FakeBook looking for secret flags
def spider():
  global ROOT, PORT, pages_to_visit, pages_visited, found_headers

  # open the initial socket connection
  s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
  s.connect((ROOT, PORT))

  # logon to the site
  logon("http://fring.ccs.neu.edu/accounts/login/?next=/fakebook/", s)

  # maintain a counter to close and reopen the connection to prevent socket errors
  count = 1
  # loop as long as we have pages to visit and haven't found all of the flags
  while pages_to_visit and len(found_headers) < 5:
    # close and reopen the connection every 3 requests
    if count % 3 == 0:
      try:
        s.shutdown(1)
        s.close()
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((ROOT, PORT))
      except:
        continue
    count += 1
    # crawl the current page
    print 'CRAWLIN ' + str(count)
    crawl('http://fring.ccs.neu.edu' + pages_to_visit[0], s)

  # print the flags after the loop completes
  for h in found_headers:
    print h

  # close the connection
  s.shutdown(1)
  s.close()


# convenience method for constructing HTTP request strings
def request(type, url, contentType=None, contentLength=None, cookies={}, data={}):
  # start by inserting the request type and desired path
  r = "%s %s HTTP/1.1\r\nhost: fring.ccs.neu.edu\r\n" % (type, url)
  # if we recieved a contentType, add the header to our request
  if contentType:
    r = r + "Content-Type: " + contentType + "\r\n"
  # if we received a contentLength, add the header to our request
  if contentLength:
    r = r + "Content-Length: " + str(contentLength) + "\r\n"
  # if we received cookies, add the header to our request
  if not len(cookies.keys()) == 0:
    r = r + "Cookie: "
    for key in cookies.keys():
      r = r + key + "=" + cookies[key] + "; "
    r = r[:-2] + "\r\n"
  # if we received data to put in the body, add it to our request
  if data:
    r = r + "\r\n" + data
  # add a CRLF at the end
  r = r + "\r\n\r\n"
  return r


# log on to the site
def logon(url, s):
  global ROOT, CSRF, USERNAME, PASSWORD

  # parse the url to get the path
  url = urlparse.urlparse(url)
  path = url.path

  # build a GET request using the path, send it, and receive the response
  req = request("GET", path)
  s.send(req)
  data = s.recv(1000000)

  # set the cookie values based on the response
  get_cookies(data)
  cookies = {'csrftoken': CSRF, 'sessionid': str(session)}

  # set the form data body of our login POST request
  login_data = 'csrfmiddlewaretoken=' + CSRF + '&next=%2Ffakebook%2F&username=' + USERNAME + '&password=' + PASSWORD
  # pass all of the arguments to request to build our POST request, send it, receive the response, and set the cookies
  req = request("POST", path, 'application/x-www-form-urlencoded', str(len(login_data)), cookies, login_data)
  s.send(req)
  time.sleep(1)
  data = s.recv(1000000)
  get_cookies(data)

  # delegate the response to the handler
  return handle(data, s, path)

# Crawl to a new page
def crawl(url, s):
  global ROOT, CSRF
  url = urlparse.urlparse(url)
  cookies = {'csrftoken': CSRF, 'sessionid': str(session)}
  # Build request string to send, includes cookies
  req = request('GET', url.path, cookies=cookies)
  s.send(req)
  data = s.recv(1000000)
  # Update cookies we're sending based on response from page
  get_cookies(data)
  return handle(data, s, url.path)

# Look at HTTP response and update our session id and CSRF if needed
def get_cookies(data):
  global CSRF, session
  # Update session id
  session_regex = r"sessionid=(.*); e"
  session_matches = re.search(session_regex, data)
  if session_matches:
    session = session_matches.group(1)
  # Update CSRF
  csrf_regex = r"csrftoken=(.*); e"
  csrf_matches = re.search(csrf_regex, data)
  if csrf_matches:
    CSRF = csrf_matches.group(1)

# Handle HTML data returned from HTTP request
def handle(data, s, path):
  my_regex = r"HTTP/1\.1 (\d*)"
  matches = re.search(my_regex, data)
  try:
    code = matches.group(1)
    if code == '302' or code == "301":
      handle_found(data, s, path)
    elif code == "200":
      handle_ok(data, s, path)
    elif code == "500":
      handle_error(data, s, path)
    elif code == "403" or code == "404":
      abort_request(data, s, path)
  except AttributeError as E:
    pass

# Handler for 403 & 404 codes
def abort_request(data, s, path):
  global pages_visited, pages_to_visit
  pages_visited.append(path)
  try:
    pages_to_visit.remove(path)
  except:
    pass

# Handler for 500 codes
def handle_error(data, s, path):
  crawl('http://fring.ccs.neu.edu' + path, s)

# Handler for 200 codes
def handle_ok(data, s, path):
  pages_visited.append(path)
  try:
    pages_to_visit.remove(path)
  except:
    pass
  parser.feed(data)

# Handler for 301 & 302 codes
def handle_found(data, s, path):
  global pages_visited, pages_to_visit
  pages_visited.append(path)
  try:
    pages_to_visit.remove(path)
  except:
    pass
  my_regex = r"Location: (http://fring\.ccs\.neu\.edu.*)\r"
  matches = re.search(my_regex, data)
  crawl(matches.group(1), s)

# Start crawling
spider()
